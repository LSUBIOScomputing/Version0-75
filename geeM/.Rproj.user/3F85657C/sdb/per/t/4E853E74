{
    "contents" : "geem <- function(formula, id,data = parent.frame(), family = gaussian, corstr = \"independence\", Mv = 1, weights = NULL, corr.mat = NULL, init.beta=NULL, init.alpha=NULL, init.phi = 1, scale.fix=FALSE, sandwich=TRUE, maxit=20, tol=0.00001){\n  call <- match.call()\n  \n  famret <- getfam(family)\n  \n  if(inherits(famret, \"family\")){\n    LinkFun <- famret$linkfun\n    InvLink <- famret$linkinv\n    VarFun <- famret$variance\n    InvLinkDeriv <- famret$mu.eta\n  }else{\n    LinkFun <- famret$LinkFun\n    VarFun <- famret$VarFun\n    InvLink <- famret$InvLink\n    InvLinkDeriv <- famret$InvLinkDeriv\n  }  \n  \n  if(scale.fix & is.null(init.phi)){\n    stop(\"If scale.fix=TRUE, then init.phi must be supplied\")\n  }\n  \n    ### First, get all the relevant elements from the arguments\n  dat <- model.frame(formula, data, na.action=na.pass)\n  nn <- dim(dat)[1]\n\n  if(typeof(data) == \"environment\"){\n    id <- id\n    weights <- weights\n    dat$id <- id\n  }\n  else{\n    if(length(call$id) == 1){\n      subj.col <- which(colnames(data) == call$id)  \n      if(length(subj.col) > 0){\n        id <- data[,subj.col]\n      }else{\n        id <- eval(call$id, envir=parent.frame())\n      }\n    }else if(is.null(call$id)){\n      id <- 1:nn\n    }\n    \n    if(length(call$weights) == 1){\n      weights.col <- which(colnames(data) == call$weights)  \n      if(length(weights.col) > 0){\n        weights <- data[,weights.col]\n      }else{\n        weights <- eval(call$weights, envir=parent.frame())\n      }\n    }else if(is.null(call$weights)){\n      weights <- NULL\n    }\n    \n  }\n\n  # W is diagonal matrix of weights, sqrtW = sqrt(W)\n  # included is diagonal matrix with 1 if weight > 0, 0 otherwise\n  # includedvec is logical vector with T if weight > 0, F otherwise\n  # Note that we need to assign weight 0 to rows with NAs \n  # in order to preserve the correlation structure\n  na.inds <- NULL\n\n  dat$id <- id  \n  if(any(is.na(dat))){\n    na.inds <- which(is.na(dat), arr.ind=T)\n  }\n  \n  if(is.null(weights)){\n    weights <- rep.int(1, nn)\n  }\n  if(!is.null(na.inds)){\n    weights[unique(na.inds[,1])] <- 0\n    for(i in unique(na.inds)[,2]){\n      if(is.factor(dat[,i])){\n        dat[na.inds[,1], i] <- levels(dat[,i])[1]  \n      }else{\n        dat[na.inds[,1], i] <- median(dat[,i], na.rm=T)\n      }\n    }\n  }\n\n  \n  includedvec <- weights>0\n\n  \n  inclsplit <- split(includedvec, id)\n\n  dropid <- NULL\n  allobs <- T\n  if(any(!includedvec)){\n    allobs <- F\n    for(i in 1:length(unique(id))){\n      if(all(!inclsplit[[i]])){\n        dropid <- c(dropid, i)\n      }    \n    }\n  }\n\n  \n  if(length(dropid)>0){\n    dropind <- which(is.element(id, dropid))\n    dat <- dat[-dropind,]\n    includedvec <- includedvec[-dropind]\n    weights <- weights[-dropind]\n    \n    id <- id[-dropind]\n  }\n  nn <- dim(dat)[1]\n  K <- length(unique(id))\n  \n  modterms <- terms(formula)\n  \n\tX <- model.matrix(formula,dat)\n\tY <- model.response(dat)\n\toffset <- model.offset(dat)\t\t\t\n\t\n\tp <- dim(X)[2]\n\n  \n\t\n\t### if no offset is given, then set to zero\n\tif(is.null(offset)){\n \t\toff <- rep(0, nn)\n\t}else{\n\t\toff <- offset\n\t}\n  \n\t# Is there an intercept column?\n\tinterceptcol <- apply(X==1, 2, all)\n\t\n\t## Basic check to see if link and variance functions make any kind of sense\n\tlinkOfMean <- LinkFun(mean(Y))\n\tif( any(is.infinite(linkOfMean) | is.nan(linkOfMean)) ){\n\t\tstop(\"Infinite or NaN in the link of the mean of responses.  Make sure link function makes sense for these data.\")\n\t}\n\tif( any(is.infinite( VarFun(mean(Y))) | is.nan( VarFun(mean(Y)))) ){\n\t\tstop(\"Infinite or NaN in the variance of the mean of responses.  Make sure variance function makes sense for these data.\")\n\t}\n\t\n\tif(is.null(init.beta)){\n\t\tif(any(interceptcol)){\n\t\t\t#if there is an intercept and no initial beta, then use link of mean of response\n\t\t\tinit.beta <- rep(0, dim(X)[2])\n\t\t\tinit.beta[which(interceptcol)] <- linkOfMean\n\t\t}else{\n\t\t\tstop(\"Must supply an initial beta if not using an intercept.\")\n\t\t}\n\t}\n  \n\n  # Number of included observations for each cluster\n  includedlen <- rep(0, K)\n  len <- rep(0,K)\n  uniqueid <- unique(id)\n\n  tmpwgt <- as.numeric(includedvec)\n  idspl <-ifelse(tmpwgt==0, NA, id)\n  includedlen <- as.numeric(summary(split(Y, idspl, drop=T))[,1])\n  len <- as.numeric(summary(split(Y, id, drop=T))[,1])\n\n  W <- Diagonal(x=weights)\n  sqrtW <- sqrt(W)\n  included <- Diagonal(x=(as.numeric(weights>0)))\n\n  # Get vector of cluster sizes... remember this len variable\n  #len <- as.numeric(summary(split(Y, id, drop=T))[,1])\n\t\n  \n\t# Figure out the correlation structure\n\tcor.vec <- c(\"independence\", \"ar1\", \"exchangeable\", \"m-dependent\", \"unstructured\", \"fixed\", \"userdefined\")\n\tcor.match <- charmatch(corstr, cor.vec)\n\t\n  if(is.na(cor.match)){stop(\"Unsupported correlation structure\")}  \n    \n\t# Set the initial alpha value\n\tif(is.null(init.alpha)){\n\t\talpha.new <- 0.2\n\t\tif(cor.match==4){\n\t\t\t# If corstr = \"m-dep\"\n\t\t\talpha.new <- 0.2^(1:Mv)\n\t\t}else if(cor.match==5){\n\t\t\t# If corstr = \"unstructured\"\n\t\t\talpha.new <- rep(0.2, sum(1:(max(len)-1)))\n\t\t}else if(cor.match==7){\n\t\t\t# If corstr = \"userdefined\"\n\t\t\talpha.new <- rep(0.2, max(unique(as.vector(corr.mat))))\n\t\t}\n\t}else{\n\t\talpha.new <- init.alpha\t\n\t}\n\t#if no initial overdispersion parameter, start at 1\n\tif(is.null(init.phi)){\n\t\tphi <- 1\n\t}else{\n    phi <- init.phi\n\t}\n\t\n\tbeta <- init.beta\n\t\n\n\t\n\t#Set up matrix storage\n\tStdErr <- Diagonal(nn)\n\tdInvLinkdEta <- Diagonal(nn)\n\tResid <- Diagonal(nn)\n\n\n\t# Initialize for each correlation structure\n\tif(cor.match == 1){\n\t\t# INDEPENDENCE\n\t\tR.alpha.inv <- Diagonal(x = rep.int(1, nn))/phi\n\t\tBlockDiag <- getBlockDiag(len)$BDiag\n\t}else if(cor.match == 2){\n\t\t# AR-1\n\t\ttmp <- buildAlphaInvAR(len)\n\t\t# These are the vectors needed to update the inverse correlation\n\t\ta1<- tmp$a1\n\t\ta2 <- tmp$a2\n\t\ta3 <- tmp$a3\n\t\ta4 <- tmp$a4\n\t\t# row.vec and col.vec for the big block diagonal of correlation inverses\n\t\t# both are vectors of indices that facilitate in updating R.alpha.inv\n\t\trow.vec <- tmp$row.vec\n\t\tcol.vec <- tmp$col.vec\n\t\tBlockDiag <- getBlockDiag(len)$BDiag\n\n\t}else if(cor.match == 3){\n\t\t# EXCHANGEABLE\n\t\t# Build a block diagonal correlation matrix for updating and sandwich calculation\n\t\t# this matrix is block diagonal with all ones.  Each block is of dimension cluster size.\n\t\ttmp <- getBlockDiag(len)\n\t\tBlockDiag <- tmp$BDiag\n\t\t\n\t\t# Create a vector of length number of observations with associated cluster size for each observation\n\t\tn.vec <- vector(\"numeric\", nn)\n\t\tindex <- c(cumsum(len) - len, nn)\n\t\tfor(i in 1:K){\n\t\t\tn.vec[(index[i]+1) : index[i+1]] <-  rep(len[i], len[i])\n\t\t}\n\t}else if(cor.match == 4){\n\t\t# M-DEPENDENT, check that M is not too large\n\t\tif(Mv >= max(len)){\n\t\t\tstop(\"Cannot estimate that many parameters: Mv >=  max(clustersize)\")\n\t\t}\n\t\t\n\t\t# Build block diagonal similar to in exchangeable case, also get row indices and column\n\t\t# indices for fast matrix updating later.\t\t\n\t\ttmp <- getBlockDiag(len)\n\t\tBlockDiag <- tmp$BDiag\n\t\trow.vec <- tmp$row.vec\n\t\tcol.vec <- tmp$col.vec\n\t\tR.alpha.inv <- NULL\n\t}else if(cor.match == 5){\n\t\t# UNSTRUCTURED\n\t\tif( max(len^2 - len)/2 > length(len)){\n\t\t\tstop(\"Cannot estimate that many parameters: not enough subjects for unstructured correlation\")\n\t\t}\n\t\ttmp <- getBlockDiag(len)\n\t\tBlockDiag <- tmp$BDiag\n\t\trow.vec <- tmp$row.vec\n\t\tcol.vec <- tmp$col.vec\t\n\t}else if(cor.match == 6){\n\t\t# FIXED\n\t\t# check if matrix meets some basic conditions\n\t\tcorr.mat <- checkFixedMat(corr.mat, len)\n\n\t\tR.alpha.inv <- as(getAlphaInvFixed(corr.mat, len), \"symmetricMatrix\")/phi\n\t\tBlockDiag <- getBlockDiag(len)$BDiag\n\t}else if(cor.match == 7){\n\t\t# USERDEFINED\n\t\tcorr.mat <- checkUserMat(corr.mat, len)\n\n\t\t# get the structure of the correlation matrix in a way that\n\t\t# I can use later on.\n\t\ttmp1 <- getUserStructure(corr.mat)\n\t\tcorr.list <- tmp1$corr.list\n\t\tuser.row <- tmp1$row.vec\n\t\tuser.col <- tmp1$col.vec\n\t\tstruct.vec <- tmp1$struct.vec\n\t\t\n\t\t# the same block diagonal trick.\n\t\ttmp2 <- getBlockDiag(len)\n\t\tBlockDiag <- tmp2$BDiag\n\t\trow.vec <- tmp2$row.vec\n\t\tcol.vec <- tmp2$col.vec\n\n\t}else if(cor.match == 0){\n\t\tstop(\"Ambiguous Correlation Structure Specification\")\n\t}else{\n\t\tstop(\"Unsupported Correlation Structure\")\t\n\t}\n\n  stop <- F\n\tconverged <- F\t\n\tcount <- 0\n\tbeta.old <- beta\n\tunstable <- F\n\tphi.old <- phi\n\t\n\n\t# Main fisher scoring loop\n\twhile(!stop){\t\t\n\t\tcount <- count+1\n\t\t\n\t\teta <- as.vector(X %*% beta) + off\n\t\t\t\t\n\t\tmu <- InvLink(eta)\n\t\t\n\t\tdiag(StdErr) <- sqrt(1/VarFun(mu))\n\t\t\n    \tif(!scale.fix){\n\t\t\t  phi <- updatePhi(Y, mu, VarFun, p, StdErr, included, includedlen)\n    \t}\n    \tphi.new <- phi\n\t\t\n                \n        ## Calculate alpha, R(alpha)^(-1) / phi\n\t\tif(cor.match == 2){\n\t\t\t# AR-1\n\t\t\talpha.new <- updateAlphaAR(Y, mu, VarFun, phi, id, len, StdErr, p, included, includedlen, includedvec, allobs)\n\t\t\tR.alpha.inv <- getAlphaInvAR(alpha.new, a1,a2,a3,a4, row.vec, col.vec)/phi\n\t\t}else if(cor.match == 3){\n\t\t\t#EXCHANGEABLE\t\t  \n\t\t\talpha.new <- updateAlphaEX(Y, mu, VarFun, phi, id, len, StdErr, Resid, p, BlockDiag, included, includedlen)\n\t\t\tR.alpha.inv <- getAlphaInvEX(alpha.new, n.vec, BlockDiag)/phi\n\t\t}else if(cor.match == 4){\n\t\t\t# M-DEPENDENT\n\t\t\tif(Mv==1){\n\t\t\t\talpha.new <- updateAlphaAR(Y, mu, VarFun, phi, id, len, StdErr, p, included, includedlen, includedvec, allobs)\n\t\t\t}else{\n\t\t\t\talpha.new <- updateAlphaMDEP(Y, mu, VarFun, phi, id, len, StdErr, Resid, p, BlockDiag, Mv, included, includedlen, allobs)\n\t\t\t\tif(sum(len>Mv) <= p){\n\t\t\t\t\tunstable <- T\n\t\t\t\t}\n\t\t\t}\n\t\t\tif(any(alpha.new >= 1)){\n\t\t\t\tstop <- T\n\t\t\t\twarning(\"some estimated correlation is greater than 1, stopping.\")\n\t\t\t}\n\t\t\tR.alpha.inv <- getAlphaInvMDEP(alpha.new, len, row.vec, col.vec)/phi\t\t\n\t\t}else if(cor.match == 5){\n\t\t\t# UNSTRUCTURED\n\t\t\talpha.new <- updateAlphaUnstruc(Y, mu, VarFun, phi, id, len, StdErr, Resid,  p, BlockDiag, included, includedlen, allobs)\n\t\t\t# This has happened to me (greater than 1 correlation estimate)\n\t\t\tif(any(alpha.new >= 1)){\n\t\t\t\tstop <- T\n\t\t\t\twarning(\"some estimated correlation is greater than 1, stopping.\")\n\t\t\t}\n\t\t\tR.alpha.inv <- getAlphaInvUnstruc(alpha.new, len, row.vec, col.vec)/phi\n\t\t}else if(cor.match ==6){\n\t\t\t# FIXED CORRELATION, DON'T NEED TO RECOMPUTE\n\t\t\tR.alpha.inv <- R.alpha.inv*phi.old/phi\n\t\t}else if(cor.match == 7){\n\t\t\t# USER SPECIFIED\n\t\t\talpha.new <- updateAlphaUser(Y, mu, phi, id, len, StdErr, Resid, p, BlockDiag, user.row, user.col, corr.list, included, includedlen, allobs)\n\t\t\tR.alpha.inv <- getAlphaInvUser(alpha.new, len, struct.vec, user.row, user.col, row.vec, col.vec)/phi\n\t\t}else if(cor.match == 1){\n\t\t\t# INDEPENDENT\n\t\t\tR.alpha.inv <-  Diagonal(x = rep.int(1/phi, nn))\n\t\t\talpha.new <- \"independent\"\n\t\t}\n\t\t\n\t\t\n\t\tbeta.list <- updateBeta(Y, X, beta, off, InvLinkDeriv, InvLink, VarFun, R.alpha.inv, StdErr, dInvLinkdEta, tol, sqrtW)\t\n\t\tbeta <- beta.list$beta\n\t\tphi.old <- phi\n\t\tif( max(abs((beta - beta.old)/(beta.old + .Machine$double.eps))) < tol ){converged <- T; stop <- T}\n\t\tif(count >= maxit){stop <- T}\t\t\n\t\tbeta.old <- beta\t\t\n\t}\n\tbiggest <- which.max(len)[1]\n\tindex <- cumsum(len[biggest])\n\tbiggest.R.alpha.inv <- R.alpha.inv[(index+1):(index+len[biggest]) , (index+1):(index+len[biggest])]\n\teta <- as.vector(X %*% beta) + off\n\tif(sandwich){\n\t\tsandvar.list <- getSandwich(Y, X, eta, id, R.alpha.inv, phi, InvLinkDeriv, InvLink, VarFun, beta.list$hess, StdErr, dInvLinkdEta, BlockDiag, sqrtW)\n\t}else{\n\t\tsandvar.list <- list()\n\t\tsandvar.list$sandvar <- \"no sandwich\"\n\t}\n\t\n\tif(!converged){warning(\"Did not converge\")}\n\tif(unstable){warning(\"Number of subjects with number of observations >= Mv is very small, some correlations are estimated with very low sample size.\")}\n\n\t\n\t# Create object of class geem with information about the fit\n  dat <- model.frame(formula, data, na.action=na.pass)\n  X <- model.matrix(formula, dat)\n  \n  if(is.character(alpha.new)){alpha.new <- 0}\n\tresults <- list()\n\tresults$beta <- as.vector(beta)\n\tresults$phi <- phi\n\tresults$alpha <- alpha.new\n\tif(cor.match == 6){\n\t\tresults$alpha <- as.vector(triu(corr.mat, 1)[which(triu(corr.mat,1)!=0)])\n\t}\n\tresults$coefnames <- colnames(X)\n\tresults$niter <- count\n\tresults$converged <- converged\n\tresults$naiv.var <- solve(beta.list$hess)*phi.new  ## call model-based\n\tresults$var <- sandvar.list$sandvar\n\tresults$call <- call\n\tresults$corr <- cor.vec[cor.match]\n\tresults$clusz <- len\n\tresults$FunList <- famret\n\tresults$X <- X\n\tresults$offset <- off\n\tresults$eta <- eta\n  results$dropped <- dropid\n  results$weights <- weights\n  results$y <- Y\n\tclass(results) <- \"geem\"\n\treturn(results)\n}\n\t\n\n\n\n\n\n### Simple moment estimator of dispersion parameter\nupdatePhi <- function(YY, mu, VarFun, p, StdErr, included, includedlen){\n\tnn <- sum(includedlen)\n\tresid <- diag(StdErr %*% included %*% Diagonal(x = YY - mu))\n\tphi <- (1/(nn-p))*crossprod(resid, resid) \n\treturn(as.numeric(phi))\t\n}\n\n### Method to update coefficients.  Goes to a maximum of 10 iterations, or when\n### rough convergence has been obtained.\nupdateBeta = function(YY, XX, beta, off, InvLinkDeriv, InvLink, VarFun, R.alpha.inv, StdErr, dInvLinkdEta, tol, sqrtW){\n\tbeta.new <- beta\n\tconv=F\n\tfor(i in 1:10){\n\t\teta <- as.vector(XX%*%beta.new) + off\n\t\t\n\t\tdiag(dInvLinkdEta) <- InvLinkDeriv(eta)\n\t\tmu <- InvLink(eta)\t\n\t\tdiag(StdErr) <- sqrt(1/VarFun(mu))\n\t\t\n\t\thess <- crossprod(sqrtW %*% StdErr %*% dInvLinkdEta %*%XX, R.alpha.inv %*% sqrtW %*% StdErr %*%dInvLinkdEta %*% XX)\n\t\testeq <- crossprod(sqrtW %*% StdErr %*%dInvLinkdEta %*%XX , R.alpha.inv %*% sqrtW %*% StdErr %*% (YY - mu))\n\n\t\tupdate <- solve(hess, esteq)\n\t\tif(max(abs(update)/beta.new) < 100*tol){break}\n\n\t\tbeta.new <- beta.new + as.vector(update)\n\t}\n\treturn(list(beta = beta.new, hess = hess))\n}\n\n### Calculate the sandiwch estimator as usual.\ngetSandwich = function(YY, XX, eta, id, R.alpha.inv, phi, InvLinkDeriv, InvLink, VarFun, hessMat, StdErr, dInvLinkdEta, BlockDiag, sqrtW){\n\n\tdiag(dInvLinkdEta) <- InvLinkDeriv(eta)\n\tmu <- InvLink(eta)\t\t\t\n\tdiag(StdErr) <- sqrt(1/VarFun(mu))\n\tscoreDiag <- Diagonal(x= YY - mu)\n\tBlockDiag <- scoreDiag %*% BlockDiag %*% scoreDiag\n\t\t\n\tnumsand <- crossprod(sqrtW %*% StdErr %*% dInvLinkdEta %*% XX, R.alpha.inv %*% sqrtW %*% StdErr %*% BlockDiag %*% StdErr %*% sqrtW %*% R.alpha.inv %*% sqrtW %*% StdErr %*% dInvLinkdEta %*% XX)\n\t\n\tsandvar <- t(solve(hessMat, numsand))\n\tsandvar <- t(solve(t(hessMat), sandvar))\n\n\treturn(list(sandvar = sandvar, numsand = numsand))\n}\n\n\n",
    "created" : 1432304421211.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3004584621",
    "id" : "4E853E74",
    "lastKnownWriteTime" : 1433274962,
    "path" : "C:/Users/lmcda4/Dropbox/geeM/Version0-75/geeM/R/geem.R",
    "project_path" : "R/geem.R",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}